{
	"name": "standardize_sweden_medicine_data",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b2c1d3c0-72b0-4173-b709-8f149fdfe038"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Unzip folder into ADLS"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import zipfile, os, shutil, time\n",
					"from io import BytesIO\n",
					"from pyspark.sql.functions import col\n",
					"\n",
					"zip_file_path = \"abfss://group3fpfilesystem@group3fpadls.dfs.core.windows.net/staging/sweden_medicine.zip\"\n",
					"local_extract = \"/tmp/unzipped\"\n",
					"dest_folder = \"abfss://group3fpfilesystem@group3fpadls.dfs.core.windows.net/staging/sweden_medicine/\"\n",
					"os.makedirs(local_extract, exist_ok=True)\n",
					"\n",
					"# load zip bytes (driver)\n",
					"zip_bytes = spark.read.format(\"binaryFile\").load(zip_file_path).collect()[0].content\n",
					"\n",
					"# get Hadoop FS handle\n",
					"fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
					"\n",
					"# Unpack and copy\n",
					"with zipfile.ZipFile(BytesIO(zip_bytes), \"r\") as z:\n",
					"    csv_entries = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
					"\n",
					"    for entry in csv_entries:\n",
					"        base = os.path.basename(entry)\n",
					"        local_path = os.path.join(local_extract, base)\n",
					"\n",
					"        # Ensure parent local folder\n",
					"        os.makedirs(os.path.dirname(local_path) or local_extract, exist_ok=True)\n",
					"\n",
					"        # Extract to local file\n",
					"        with z.open(entry) as src, open(local_path, \"wb\") as dst:\n",
					"            shutil.copyfileobj(src, dst)\n",
					"\n",
					"        # Copy local file -> ADLS using Hadoop FS (copyFromLocalFile)\n",
					"        src_path = spark._jvm.org.apache.hadoop.fs.Path(\"file://\" + local_path)\n",
					"        dst_path = spark._jvm.org.apache.hadoop.fs.Path(dest_folder + base)\n",
					"\n",
					"        fs.copyFromLocalFile(False, True, src_path, dst_path)\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Create dataframes out of correct CSV files"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Create spark dataframes out of CSVs\n",
					"df_dispensed_units = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"abfss://group3fpfilesystem@group3fpadls.dfs.core.windows.net/staging/sweden_medicine/läkemedel - data - antal expedieringar - 2006-2024.csv\")\n",
					"df_users = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"abfss://group3fpfilesystem@group3fpadls.dfs.core.windows.net/staging/sweden_medicine/läkemedel - data - antal patienter - 2006-2024.csv\")\n",
					"\n",
					"df_dispensed_units.show(5)\n",
					"df_users.show(5)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Filter data to include correct measure, all ages and both sexes"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# List of relevant ATC codes\n",
					"atc_codes = [\n",
					"    'A10AB01','A10AB03','A10AB04','A10AB05','A10AB06',\n",
					"    'A10AC01','A10AC03','A10AC30','A10AD01','A10AD03',\n",
					"    'A10AD04','A10AD05','A10AE01','A10AE02','A10AE04',\n",
					"    'A10AE05','A10AE06','A10AE54','A10AE56','A10BA02',\n",
					"    'A10BB01','A10BB02','A10BB07','A10BB12','A10BD03',\n",
					"    'A10BD04','A10BD05','A10BD07','A10BD08','A10BD10',\n",
					"    'A10BD11','A10BD15','A10BD19','A10BD20','A10BD21',\n",
					"    'A10BD23','A10BD24','A10BF01','A10BG01','A10BG02',\n",
					"    'A10BG03','A10BH01','A10BH02','A10BH03','A10BH05',\n",
					"    'A10BJ01','A10BJ02','A10BJ03','A10BJ05','A10BJ06',\n",
					"    'A10BK01','A10BK02','A10BK03','A10BK04','A10BX02','A10BX03'\n",
					"    ]\n",
					"\n",
					"# Filter dispensed units dataframe\n",
					"df_dispensed_units_filtered = df_dispensed_units.filter(\n",
					"    (col(\"Mått\") == 3) &\n",
					"    (col(\"Region\") == 0) &\n",
					"    (col(\"ATC-kod\").isin(atc_codes)) &\n",
					"    (col(\"Ålder\") == 99) &\n",
					"    (col(\"Kön\") == 3)\n",
					")\n",
					"\n",
					"# Filter users dataframe\n",
					"df_users_filtered = df_users.filter(\n",
					"    (col(\"Mått\") == 1) &\n",
					"    (col(\"Region\") == 0) &\n",
					"    (col(\"ATC-kod\").isin(atc_codes)) &\n",
					"    (col(\"Ålder\") == 99) &\n",
					"    (col(\"Kön\") == 3)\n",
					")\n",
					"\n",
					"df_dispensed_units_filtered.show(5)\n",
					"df_users_filtered.show(5)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Remove unnecessary and rename columns"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Remove unnecessary columns\n",
					"df_dispensed_units_minimal = df_dispensed_units_filtered[[\"År\", \"ATC-kod\", \"Värde\"]]\n",
					"df_users_minimal = df_users_filtered[[\"År\", \"ATC-kod\", \"Värde\"]]\n",
					"\n",
					"# Rename dispensed units dataframe columns\n",
					"df_dispensed_units_renamed = (\n",
					"    df_dispensed_units_minimal\n",
					"    .withColumnRenamed(\"År\", \"year\")\n",
					"    .withColumnRenamed(\"ATC-kod\", \"atc_level_5\")\n",
					"    .withColumnRenamed(\"Värde\", \"dispensed_units\")\n",
					")\n",
					"\n",
					"# Rename users dataframe columns\n",
					"df_users_renamed = (\n",
					"    df_users_minimal\n",
					"    .withColumnRenamed(\"År\", \"year\")\n",
					"    .withColumnRenamed(\"ATC-kod\", \"atc_level_5\")\n",
					"    .withColumnRenamed(\"Värde\", \"users\")\n",
					")\n",
					"\n",
					"df_dispensed_units_renamed.show(5)\n",
					"df_users_renamed.show(5)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Inner join tables"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"combined = df_dispensed_units_renamed \\\n",
					"    .join(df_users_renamed, on=[\"year\", \"atc_level_5\"], how=\"inner\")\n",
					"\n",
					"combined.show(5)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Sink to ADLS"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"combined.write.mode(\"overwrite\").parquet(\"abfss://group3fpfilesystem@group3fpadls.dfs.core.windows.net/standardized/sweden_medicine_2.parquet\")"
				],
				"execution_count": 6
			}
		]
	}
}